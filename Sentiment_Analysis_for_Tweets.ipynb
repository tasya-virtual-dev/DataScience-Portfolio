{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "colab": {
      "name": "Sentiment Analysis for Tweets.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQIuoJ0rkz8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "dictionary = set(nltk.corpus.words.words()) #To be used for MaxMatch\n",
        "\n",
        "#Function to lemmatize word | Used during maxmatch\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word,'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word,'n')\n",
        "    return lemma\n",
        "\n",
        "#Function to implement the maxmatch algorithm for multi-word hashtags\n",
        "def maxmatch(word,dictionary):\n",
        "    if not word:\n",
        "        return []\n",
        "    for i in range(len(word),1,-1):\n",
        "        first = word[0:i]\n",
        "        rem = word[i:]\n",
        "        if lemmatize(first).lower() in dictionary: #Important to lowercase lemmatized words before comparing in dictionary. \n",
        "            return [first] + maxmatch(rem,dictionary)\n",
        "    first = word[0:1]\n",
        "    rem = word[1:]\n",
        "    return [first] + maxmatch(rem,dictionary)\n",
        "\n",
        "#Function to preprocess a single tweet\n",
        "def preprocess(tweet):\n",
        "    \n",
        "    tweet = re.sub(\"@\\w+\",\"\",tweet).strip()\n",
        "    tweet = re.sub(\"http\\S+\",\"\",tweet).strip()\n",
        "    hashtags = re.findall(\"#\\w+\",tweet)\n",
        "    \n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(\"#\\w+\",\"\",tweet).strip() \n",
        "    \n",
        "    hashtag_tokens = [] #Separate list for hashtags\n",
        "    \n",
        "    for hashtag in hashtags:\n",
        "        hashtag_tokens.append(maxmatch(hashtag[1:],dictionary))        \n",
        "    \n",
        "    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    segmented_sentences = segmenter.tokenize(tweet)\n",
        "    \n",
        "    #General tokenization\n",
        "    processed_tweet = []\n",
        "    \n",
        "    word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
        "    for sentence in segmented_sentences:\n",
        "        tokenized_sentence = word_tokenizer.tokenize(sentence.strip())\n",
        "        processed_tweet.append(tokenized_sentence)\n",
        "    \n",
        "    #Processing the hashtags only when they exist in a tweet\n",
        "    if hashtag_tokens:\n",
        "        for tag_token in hashtag_tokens:\n",
        "            processed_tweet.append(tag_token)\n",
        "    \n",
        "    return processed_tweet\n",
        "    \n",
        "#Custom function that takes in a file, and passes each tweet to the preprocessor\n",
        "def preprocess_file(filename):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    f = open(filename)\n",
        "    for line in f:\n",
        "        tweet_dict = json.loads(line)\n",
        "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
        "        labels.append(int(tweet_dict[\"label\"]))\n",
        "    return tweets,labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENs_fkyakz89",
        "colab_type": "code",
        "outputId": "8f78c0b2-e788-4ae7-909a-56e4eff32497",
        "colab": {}
      },
      "source": [
        "maxmatch('wecan',dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'can']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYbR6SzJkz9J",
        "colab_type": "code",
        "outputId": "fc6417a2-e6b6-4546-b942-5ba9ca9f5c49",
        "colab": {}
      },
      "source": [
        "maxmatch('casestudy',dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cases', 'tu', 'd', 'y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLpO8Filkz9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = preprocess_file('data/sentiment/training.json')\n",
        "train_tweets = train_data[0]\n",
        "train_labels = train_data[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV4tbOZNkz9k",
        "colab_type": "code",
        "outputId": "3b72969c-7d8e-484b-9c14-e0db1c1e939c",
        "colab": {}
      },
      "source": [
        "print train_tweets[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[u'dear', u'the', u'newooffice', u'for', u'mac', u'is', u'great', u'and', u'all', u',', u'but', u'no', u'lync', u'update', u'?'], [u'c', u\"'\", u'mon', u'.']], [[u'how', u'about', u'you', u'make', u'a', u'system', u'that', u'doesn', u\"'\", u't', u'eat', u'my', u'friggin', u'discs', u'.'], [u'this', u'is', u'the', u'2nd', u'time', u'this', u'has', u'happened', u'and', u'i', u'am', u'so', u'sick', u'of', u'it', u'!']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_8u8jTvkz9t",
        "colab_type": "code",
        "outputId": "b4675e7b-5186-4dc5-ab93-790d49e8fd6f",
        "colab": {}
      },
      "source": [
        "#Printing examples of multi-word hashtags (Doesn't work for multi sentence tweets)\n",
        "f = open('data/sentiment/training.json')\n",
        "count = 1\n",
        "for index,line in enumerate(f):\n",
        "    if count >5:\n",
        "        break\n",
        "    original_tweet = json.loads(line)[\"text\"]\n",
        "    hashtags = re.findall(\"#\\w+\",original_tweet)\n",
        "    if hashtags:\n",
        "        for hashtag in hashtags:\n",
        "            if len(maxmatch(hashtag[1:],dictionary)) > 1:\n",
        "                #If the length of the array returned by the maxmatch function is greater than 1,\n",
        "                #it means that the algorithm has detected a hashtag with more than 1 word inside. \n",
        "                print str(count) + \". Original Tweet: \" + original_tweet + \"\\nProcessed tweet: \" + str(train_tweets[index]) + \"\\n\"\n",
        "                count += 1\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Original Tweet: If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
            "Processed tweet: [[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.'], [u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?'], [u'windows', u'1', u'0'], [u'x', u'box', u'one']]\n",
            "\n",
            "2. Original Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
            "Processed tweet: [[u'microsoft', u',', u'i', u'may', u'not', u'prefer', u'your', u'gaming', u'branch', u'of', u'business', u'.'], [u'but', u',', u'you', u'do', u'make', u'a', u'damn', u'fine', u'operating', u'system', u'.'], [u'Window', u's', u'1', u'0']]\n",
            "\n",
            "3. Original Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
            "Processed tweet: [[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.'], [u'Window', u's', u'1', u'0'], [u'Window', u's', u'1', u'0', u'fail']]\n",
            "\n",
            "4. Original Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
            "Processed tweet: [[u'2nd', u'computer', u'with', u'same', u'error', u'!!!'], [u'guess', u'we', u'will', u'shelve', u'this', u'until', u'sp1', u'!'], [u'Window', u's', u'1', u'0', u'fail']]\n",
            "\n",
            "5. Original Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
            "Processed tweet: [[u'sunday', u'morning', u',', u'quiet', u'day', u'so', u'time', u'to', u'welcome', u'in'], [u'Window', u's', u'1', u'0']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFK6XygXkz92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#To identify words appearing less than n times, we're creating a dictionary for the whole training set\n",
        "\n",
        "total_train_bow = {}\n",
        "\n",
        "for tweet in train_tweets:\n",
        "    for segment in tweet:\n",
        "        for token in segment:\n",
        "            total_train_bow[token] = total_train_bow.get(token,0) + 1\n",
        "\n",
        "#Function to convert pre_processed tweets to bag of words feature dictionaries\n",
        "#Allows for options to remove stopwords, and also to remove words occuring less than n times in the whole training set.            \n",
        "def convert_to_feature_dicts(tweets,remove_stop_words,n): \n",
        "    feature_dicts = []\n",
        "    for tweet in tweets:\n",
        "        # build feature dictionary for tweet\n",
        "        feature_dict = {}\n",
        "        if remove_stop_words:\n",
        "            for segment in tweet:\n",
        "                for token in segment:\n",
        "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
        "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
        "        else:\n",
        "            for segment in tweet:\n",
        "                for token in segment:\n",
        "                    if n<=0 or total_train_bow[token]>=n:\n",
        "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
        "        feature_dicts.append(feature_dict)\n",
        "    return feature_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LydTR9IGkz-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "vectorizer = DictVectorizer()\n",
        "\n",
        "#Conversion to feature dictionaries\n",
        "train_set = convert_to_feature_dicts(train_tweets,True,2)\n",
        "\n",
        "dev_data = preprocess_file('data/sentiment/develop.json')\n",
        "\n",
        "dev_set = convert_to_feature_dicts(dev_data[0],False,0)\n",
        "\n",
        "#Conversion to sparse representations\n",
        "training_data = vectorizer.fit_transform(train_set)\n",
        "\n",
        "development_data = vectorizer.transform(dev_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVxl0OFikz-I",
        "colab_type": "code",
        "outputId": "f2dfb44b-7f9b-472a-e6d5-cca73b9cbbc9",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import cross_validation\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "\n",
        "#Grid used to test the combinations of parameters\n",
        "tree_param_grid = [\n",
        "    {'criterion':['gini','entropy'], 'min_samples_leaf': [75,100,125,150,175], 'max_features':['sqrt','log2',None],\n",
        "    }\n",
        "]\n",
        "\n",
        "tree_clf = GridSearchCV(DecisionTreeClassifier(),tree_param_grid,cv=10,scoring='accuracy')\n",
        "\n",
        "tree_clf.fit(training_data,train_data[1])\n",
        "\n",
        "print \"Optimal parameters for DT: \" + str(tree_clf.best_params_) #To print out the best discovered combination of the parameters\n",
        "\n",
        "tree_predictions = tree_clf.predict(development_data)\n",
        "\n",
        "print \"\\nDecision Tree Accuracy: \" + str(accuracy_score(dev_data[1],tree_predictions))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal parameters for DT: {'max_features': None, 'criterion': 'entropy', 'min_samples_leaf': 75}\n",
            "\n",
            "Decision Tree Accuracy: 0.487151448879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUicGUqAkz-R",
        "colab_type": "code",
        "outputId": "dd075d4b-3bfe-4393-8a72-889e6214cf0d",
        "colab": {}
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "#The dummy classifier below always predicts the most frequent class, as specified in the strategy. \n",
        "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_clf.fit(development_data,dev_data[1])\n",
        "dummy_predictions = dummy_clf.predict(development_data)\n",
        "\n",
        "print \"\\nMost common class baseline accuracy: \" + str(accuracy_score(dev_data[1],dummy_predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Most common class baseline accuracy: 0.420448332422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2YPanCPkz-b",
        "colab_type": "code",
        "outputId": "b2382a7e-786d-4aed-c0d2-0a777925bd64",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_param_grid = [\n",
        "    {'C':[0.012,0.0125,0.130,0.135,0.14],\n",
        "     'solver':['lbfgs'],'multi_class':['multinomial']\n",
        "    }\n",
        "]\n",
        "\n",
        "log_clf = GridSearchCV(LogisticRegression(),log_param_grid,cv=10,scoring='accuracy')\n",
        "\n",
        "log_clf.fit(training_data,train_data[1])\n",
        "\n",
        "log_predictions = log_clf.predict(development_data)\n",
        "\n",
        "print \"Optimal parameters for LR: \" + str(log_clf.best_params_)\n",
        "\n",
        "print \"Logistic Regression Accuracy: \" + str(accuracy_score(dev_data[1],log_predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal parameters for LR: {'multi_class': 'multinomial', 'C': 0.012, 'solver': 'lbfgs'}\n",
            "Logistic Regression Accuracy: 0.493165664297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLMCTXhjkz-m",
        "colab_type": "text"
      },
      "source": [
        "## Polarity Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg7P-8mpkz-r",
        "colab_type": "code",
        "outputId": "ab090376-a600-4f24-d7e4-1b943855bb45",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "import random\n",
        "\n",
        "swn_positive = []\n",
        "\n",
        "swn_negative = []\n",
        "\n",
        "#Function supplied with the assignment, not described below.\n",
        "def get_polarity_type(synset_name):\n",
        "    swn_synset =  swn.senti_synset(synset_name)\n",
        "    if not swn_synset:\n",
        "        return None\n",
        "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
        "        return 1\n",
        "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "for synset in wn.all_synsets():      \n",
        "    \n",
        "    # count synset polarity for each lemma\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    neutral_count = 0\n",
        "    \n",
        "    for lemma in synset.lemma_names():\n",
        "        for syns in wn.synsets(lemma):\n",
        "            if get_polarity_type(syns.name())==1:\n",
        "                pos_count+=1\n",
        "            elif get_polarity_type(syns.name())==-1:\n",
        "                neg_count+=1\n",
        "            else:\n",
        "                neutral_count+=1\n",
        "    \n",
        "    if pos_count > neg_count and pos_count >= neutral_count: #>=neutral as words that are more positive than negative, \n",
        "                                                                #despite being equally neutral might belong to positive list (explain)\n",
        "        swn_positive.append(synset.lemma_names()[0])\n",
        "    elif neg_count > pos_count and neg_count >= neutral_count:\n",
        "        swn_negative.append(synset.lemma_names()[0])       \n",
        "\n",
        "swn_positive = list(set(swn_positive))\n",
        "swn_negative = list(set(swn_negative))\n",
        "            \n",
        "            \n",
        "print 'Positive words: ' + str(random.sample(swn_positive,5))\n",
        "\n",
        "print 'Negative Words: ' + str(random.sample(swn_negative,5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive words: [u'mercy', u'prudent', u'blue_ribbon', u'synergistically', u'controversial']\n",
            "Negative Words: [u'gynobase', u'anger', u'unservile', u'intestate', u'paresthesia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ezoXtOwkz-2",
        "colab_type": "code",
        "outputId": "ab86ae0b-5615-4141-8bc5-d0abd04dcda5",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from nltk.data import find\n",
        "import random\n",
        "\n",
        "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
        "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
        "\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample,binary=False)\n",
        "\n",
        "wv_positive = []\n",
        "wv_negative = []\n",
        "\n",
        "for word in model.vocab:\n",
        "    try:\n",
        "        word=word.lower()\n",
        "    \n",
        "        pos_score = 0.0\n",
        "        neg_score = 0.0\n",
        "    \n",
        "        for seed in positive_seeds:\n",
        "            pos_score = pos_score + model.similarity(word,seed)\n",
        "    \n",
        "        for seed in negative_seeds:\n",
        "            neg_score = neg_score + model.similarity(word,seed)\n",
        "        \n",
        "        avg = (pos_score - neg_score)/16 #Total number of seeds is 16\n",
        "    \n",
        "        if avg>0.03:\n",
        "            wv_positive.append(word)\n",
        "        elif avg<-0.03:\n",
        "            wv_negative.append(word)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "\n",
        "print 'Positive words: ' + str(random.sample(wv_positive,5))\n",
        "\n",
        "print 'Negative Words: ' + str(random.sample(wv_negative,5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive words: [u'hoping', u'treble', u'revolutionary', u'sumptuous', u'productive']\n",
            "Negative Words: [u'lawless', u'trudged', u'perpetuation', u'mystified', u'tendency']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgg0KgfAkz_A",
        "colab_type": "code",
        "outputId": "88d10f3e-2edb-4ac6-d5d1-e888851a12b3",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import opinion_lexicon\n",
        "import math\n",
        "from __future__ import division\n",
        "\n",
        "positive_words = opinion_lexicon.positive()\n",
        "negative_words = opinion_lexicon.negative()\n",
        "\n",
        "#Calculate the percentage of words in the manually annotated lexicon set, that also appear in an automatic lexicon.\n",
        "def get_perc_manual(manual_pos,manual_neg,auto_pos,auto_neg):\n",
        "    return len(set(manual_pos+manual_neg).intersection(set(auto_pos+auto_neg)))/len(manual_pos+manual_neg)*100\n",
        "\n",
        "print \"% of words in manual lexicons, also present in the automatic lexicon\"\n",
        "print \"First automatic lexicon: \"+ str(get_perc_manual(positive_words,negative_words,swn_positive,swn_negative))\n",
        "print \"Second automatic lexicon: \"+ str(get_perc_manual(positive_words,negative_words,wv_positive,wv_negative))\n",
        "\n",
        "#Calculate the accuracy of words in the automatic lexicon. Assuming that the manual lexicons are accurate, it calculates the percentage of words that occur in both positive and negative (respectively) lists of automatic and manual lexicons.\n",
        "def get_lexicon_accuracy(manual_pos,manual_neg,auto_pos,auto_neg):\n",
        "    common_words = set(manual_pos+manual_neg).intersection(set(auto_pos+auto_neg))-set(negative_seeds)-set(positive_seeds)\n",
        "    return (len(set(manual_pos) & set(auto_pos) & common_words)+len(set(manual_neg) & set(auto_neg) & common_words))/len(common_words)*100\n",
        "\n",
        "print \"\\nAccuracy of lexicons: \"\n",
        "print \"First automatic lexicon: \"+ str(get_lexicon_accuracy(positive_words,negative_words,swn_positive,swn_negative))\n",
        "print \"Second automatic lexicon: \"+ str(get_lexicon_accuracy(positive_words,negative_words,wv_positive,wv_negative))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of words in manual lexicons, also present in the automatic lexicon\n",
            "First automatic lexicon: 7.42377375166\n",
            "Second automatic lexicon: 37.7964354102\n",
            "\n",
            "Accuracy of lexicons: \n",
            "First automatic lexicon: 82.4701195219\n",
            "Second automatic lexicon: 98.9415915327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0bn9DGzkz_N",
        "colab_type": "code",
        "outputId": "bfcfcc9a-1606-473d-cc28-e5a9725ce7fb",
        "colab": {}
      },
      "source": [
        "#All lexicons are converted to sets for faster preprocessing.\n",
        "manual_pos_set = set(positive_words)\n",
        "manual_neg_set = set(negative_words)\n",
        "\n",
        "syn_pos_set = set(swn_positive)\n",
        "syn_neg_set = set(swn_negative)\n",
        "\n",
        "wordvec_pos_set = set(wv_positive)\n",
        "wordvec_neg_set = set(wv_negative)\n",
        "\n",
        "#Function to calculate the polarity score of a sentence based on the frequency of positive or negative words. \n",
        "def get_polarity_score(sentence,pos_lexicon,neg_lexicon):\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    for word in sentence:\n",
        "        if word in pos_lexicon:\n",
        "            pos_count+=1\n",
        "        if word in neg_lexicon:\n",
        "            neg_count+=1\n",
        "    if pos_count>neg_count:\n",
        "        return 1\n",
        "    elif neg_count>pos_count:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "\n",
        "#Function to calculate the score for each tweet, and compare it against the actual labels of the dataset and calculate/count the accuracy score. \n",
        "def data_polarity_accuracy(dataset,datalabels,pos_lexicon,neg_lexicon):\n",
        "    accuracy_count = 0\n",
        "    for index,tweet in enumerate(dataset):\n",
        "        if datalabels[index]==get_polarity_score([word for sentence in tweet for word in sentence],pos_lexicon,neg_lexicon):\n",
        "            accuracy_count+=1\n",
        "    return (accuracy_count/len(dataset))*100\n",
        "        \n",
        "print \"Manual lexicon accuracy: \"+str(data_polarity_accuracy(dev_data[0],dev_data[1],manual_pos_set,manual_neg_set))      \n",
        "print \"First auto lexicon accuracy: \"+str(data_polarity_accuracy(dev_data[0],dev_data[1],syn_pos_set,syn_neg_set))      \n",
        "print \"Second auto lexicon accuracy: \"+str(data_polarity_accuracy(dev_data[0],dev_data[1],wordvec_pos_set,wordvec_neg_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manual lexicon accuracy: 45.2159650082\n",
            "First auto lexicon accuracy: 38.9283761618\n",
            "Second auto lexicon accuracy: 45.1612903226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQFbFbiXkz_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_feature_dicts_v2(tweets,manual,first,second,remove_stop_words,n): \n",
        "    feature_dicts = []\n",
        "    for tweet in tweets:\n",
        "        # build feature dictionary for tweet\n",
        "        feature_dict = {}\n",
        "        if remove_stop_words:\n",
        "            for segment in tweet:\n",
        "                for token in segment:\n",
        "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
        "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
        "        else:\n",
        "            for segment in tweet:\n",
        "                for token in segment:\n",
        "                    if n<=0 or total_train_bow[token]>=n:\n",
        "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
        "        if manual == True:\n",
        "            feature_dict['manual_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],manual_pos_set,manual_neg_set)\n",
        "        if first == True:\n",
        "            feature_dict['synset_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],syn_pos_set,syn_neg_set)\n",
        "        if second == True:\n",
        "            feature_dict['wordvec_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],wordvec_pos_set,wordvec_neg_set)\n",
        "    \n",
        "        feature_dicts.append(feature_dict)      \n",
        "    return feature_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z84_wMCfkz_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_v2 = convert_to_feature_dicts_v2(train_tweets,True,False,True,True,2)\n",
        "\n",
        "training_data_v2 = vectorizer.fit_transform(training_set_v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpqniqHbkz_i",
        "colab_type": "code",
        "outputId": "ebf2f496-3ee6-42a8-d000-0e1cdb1d6946",
        "colab": {}
      },
      "source": [
        "dev_set_v2 = convert_to_feature_dicts_v2(dev_data[0],True,False,True,False,0)\n",
        "\n",
        "development_data_v2 = vectorizer.transform(dev_set_v2)\n",
        "\n",
        "log_clf_v2 = LogisticRegression(C=0.012,solver='lbfgs',multi_class='multinomial')\n",
        "\n",
        "log_clf_v2.fit(training_data_v2,train_data[1])\n",
        "\n",
        "log_predictions_v2 = log_clf_v2.predict(development_data_v2)\n",
        "\n",
        "print \"Logistic Regression V2 (with polarity scores) Accuracy: \" + str(accuracy_score(dev_data[1],log_predictions_v2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression V2 (with polarity scores) Accuracy: 0.507927829415\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}